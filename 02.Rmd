# Applying MLM to Longitudinal Data 

## Overview 

Getting the “right” growth curve is critical since it can lead to misleading results if it is incorrect. So, graph the outcome data and make sure you understand the “growth curve” for individuals. Use “Plots” in “Univariate ANOVA” to look at what the growth curve looks like. 

Also, MLM assumes multivariate normality, so we want to be sure that our data approaches normality (robust standard errors are available in Generalized Linear mixed models, which we will talk about later in the semester). They are robust to violations of multivariate normality. Also check for outliers. Use scatterplots and frequencies to look for outliers, skewness, normality.

Level-2 equations predict between person differences in the level-1 parameters (e.g., intercept and slope) in the level-1 growth curve model. Thus, the level-2 equations can be used to test if individual characteristics (e.g., treatment condition, demographic information, personality traits, etc.) predict these level-1 (within person) relations, answering questions like: is treatment condition significantly related to slope of change over time (b1).

 - If you have a priori hypotheses (like a treatment condition effect that you want to test), you can go directly to testing that.
 - If you do not have a priori hypotheses, you usually look at the variance of the random effect for the slope, and if it is significant, then that means that there is significant variability in the slope between different people, so it is worthwhile to investigate predictors of that variability.
 - How do you test for variability in the slope? You can test the significance of $\mu_{1i}$.

When you substitute the level 2 equations into the level 1 model, you get the composite model that is estimated using maximum likelihood (ML) estimation. 
 
```{r, echo = FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/carolinelee78/mlmbookdown/main/_figures/IMG5.jpg")
```

Note that since  there is an interaction between Time and Condition, the main effects for Time and Condition do not reflect the “average” effect of Time or Condition, but rather the effect of one when the other is 0. Thus, if Time is “centered” at assessment 1 (baseline), then Timec1=0 is the baseline assessment and the effect of condition is thus the Condition effect at baseline, which you hope to be non-significant. The effect of Time in this model is the slope for those in Condition=0 (hopefully you do not code condition =1 or 2, since then the slope of Time (the main effect) is nonsensical).

One often runs the model twice, once for each treatment condition equaling 0, to get the slope (and significance) of Time for both treatment conditions. You can also run the model with treatment condition coded -.5, .5 to get the “average” effect of Time across both treatment conditions. This latter coding is not often done when treatment condition is the level 2 variable, since researchers rarely want to know about the average of placebo and treatment. It is more frequently done when the level 2 predictor of slope is a variable like gender, when you want to get the average slope across genders.

One also often runs the model with Time centered at various assessments to obtain the significance of treatment condition at the various assessments. Thus, if the variable Index1 codes the 4 assessments as 1,2,3,4, one might run the model alternately with Time centered at baseline (Timec1=Index1-1), at post-treatment (Timec3=Index1-3), and at FU (Timec4=Index1-4).

Proof that using Timec4 will give you condition differences at follow-up: 

$$ Y = b_0 + b_1*Cond + b_2*Timec4 + b_3*Cond*Timec4 $$
$$ Y (Timec4 = 0) = b_0 + b_1*Cond + b_2*0 + b_3*Cond*0$$
$$= b_0 + b_1*Cond$$
So the main effect for condition when time is centered at assessment 4 is the effect of condition at assessment 4 (FU; follow-up). Note the difference between that and the prediction of Y (using this same coding of time) at time 3:

$$ Y(Timec4 = 1) = b_0 + b_1*Cond + b_2*(-1) + b_3*(-1)*Cond $$
$$ = (b_0-b_2)+(b_1-b_3)*Cond $$

## Homework 2 - 08/31

Use the restructured OCD data file to rerun the analysis, using week (weeks from pre) as the time variable instead of index1. Use either a series of "computes" or use a "recode" to create the week variable. The "pre" assessment (index1=1) should be coded week=0. The week4 assessment (index1=2) should be coded week=4, etc. Then use condition, week, and condition x week as the fixed effects, and include a random effect for the intercept and for week (always use an unstructured covariance matrix for the random effects). Compare the results from this model to the results from the model we did in class.

**SPSS**

```
*STEP 1. Recode index1 into week as the time variable. 

RECODE Index1 (1=0) (2=4) (3=16) (4=26) (5=52) (MISSING=SYSMIS) INTO week.
EXECUTE.
```

```
* STEP 2. Center week at week52.

COMPUTE week52=week-52.
```

```
* STEP 3. Run model. 

MIXED YBOC WITH condition week52
  /CRITERIA=DFMETHOD(SATTERTHWAITE) CIN(95) MXITER(1000) MXSTEP(100) SCORING(1)
    SINGULAR(0.000000000001) HCONVERGE(0.00000001, RELATIVE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0,
    ABSOLUTE)
  /FIXED=condition week52 condition*week52 | SSTYPE(3)
  /METHOD=REML
  /PRINT=SOLUTION TESTCOV
  /RANDOM=INTERCEPT week52 | SUBJECT(Subject#) COVTYPE(UN)
  /REPEATED=week | SUBJECT(Subject#) COVTYPE(ID).
```
  
```{r, echo = FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/carolinelee78/mlmbookdown/main/_figures/IMG8.png")
```

**Interpretation of** _**week52**_ **Main Effect** 

There is a significant difference on the effect of time on YBOC (decline for when condition = 0).

**Interpretation of** _**condition**_ **Main Effect**

Centering at assessment 52, there’s no significant effect of condition. 

**Interpretation of the Interaction Effect** 

The change in differences across conditions in terms of YBOCS over time is significant. 

```{r, echo = FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/carolinelee78/mlmbookdown/main/_figures/IMG9.png")
```

**Interpretation of Covariance Parameters** 

- .563 is the variability of the individual scores around the estimated average of their conditions at the last time point (UN1.1).
- An individual’s average slope does not vary around the average slope (UN2.2 – slope random effect – within each condition. Deviation for the average slope).
- The higher the slope, the higher the intercept for each individual on average as this is a positive value for (UN2.1 - covariance).

--- 

**R**

```{r, warning = FALSE, results= 'hide'}
library(haven)
library(lme4)
library(dplyr)
```

```{r}
DF1 <- read_sav("Restructured_OCDMediationYBOCSBDI1inS.sav")
DF1$week <- dplyr::recode(DF1$Index1, `1` = 0,
                                      `2` = 4,
                                      `3` = 16,
                                      `4` = 26, 
                                      `5` = 52)

DF1$Subject <- DF1$`Subject#`

model1 <- lmer(YBOC ~ week + condition + week*condition + (1 + week || Subject), data = DF1)
summary(model1, corr = FALSE)
 
```

```{r}
DF1$week52 <- DF1$week - 52 

model2 <- lmer(YBOC ~ week52 + condition + week52*condition + (1 | week52), data = DF1)

summary(model2, corr = FALSE)
```

## Quadratic Growth Curve Models 

A **quadratic model of change** includes a quadratic effect of time (i.e., as $time^2$, otherwise known as the _quadratic rate of change_, or the _quadratic slope for time_) in addition to an intercept and a linear effect of time (i.e., as $time^1$, otherwise known as the _linear rate of change_, or the _linear slope for time_). A quadratic effect of time implies a _change_ in the linear rate of change over time, such that the linear time slope either *accelerates* (speeds up) or *decelerates* (slows down) over time. 

The figure below illustrates how positive or negative linear or quadratic effects of time can work together to create many possible patterns of nonlinear change. First, the top left shows how a _positive_ linear effect of time and a _positive_ quadratic effect of time create an **accelerating positive function**, or a mean trajectory in which the rate of increase speeds up over time. Second, the top right shows how a _positive_ linear effect of time and a _negative_ quadratic effect of time create a **decelerating positive function**, or a mean trajectory in which the rate of increase slows down over time instead. Third, the bottom left shows how a _negative_ linear effect of time and a _positive_ quadratic effect of time create a **decelerating negative function**, or a mean trajectory in which the rate of decrease slows down over time. Finally, the bottom right shows how a _negative_ linear effect of time and a _negative_ quadratic effect of time create an **accelerating negative function**, or a mean trajectory in which the rate of decrease speeds over time. We cannot determine just from its sign if a quadratic effect creates acceleration or deceleration. A _positive_ quadratic effect of time can make a positive linear effect of time _more positive_ (acceleration), or it can make a negative linear effect of time _less negative_ (deceleration). Similarly, a _negative_ quadratic effect of time can make a negative linear effect of time _more negative_ (acceleration). 

Furthermore, just like any other predictor that varies over time, a quadratic effect of time can have both a fixed effect and a random effect. That is, a **fixed quadratic effect of time** in the model for the means implies a change in the linear rate of change _on average_. More specifically, 
```{r, echo = FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/carolinelee78/mlmbookdown/main/_figures/IMG7.jpg")
```

Typically, in a treatment outcome study, the improvement in outcome over time levels off. Hence, we need to be able to model growth as curvilinear. We can do this by looking at the pattern of means across time, within condition, to get an idea of what the change in the means over time looks like. 
 - Remember that this is the pattern of means for the non-missing data, so it may not reflect MLM growth curves. 
 
We can test for a quadratic curve by adding a "time squared" term to the level-1 MLM model. Note that we have a level-2 equation for every level 1 parameter, so with a quadratic growth curve, you will have 3 level-2 equations, adding the level-2 equation for the timesq term. The level 2 equation for the quadratic term does not *have* to have the same predictors that the linear time term has, but if it does have predictors, they must be included as predictors of linear time and the intercept. This is necessary to have all subcomponents of the interaction included in the model.

```{r, echo = FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/carolinelee78/mlmbookdown/main/_figures/IMG6.jpg")
```
Be sure that the squared term uses the exact linear term, squared. Don’t use timec1 for the linear term and timec5-squared for the squared term. Otherwise, the meaning of the some of the coefficients (e.g., main effects) can become problematic. Even higher level terms (e.g., cubic, quartic, etc.) can also be added if needed.

### Meaning of the Linear vs. Quadratic Terms

- When using a quadratic growth curve model, the regression coefficient for the linear term is the tangent to the curve at Time=0. 
- This makes sense when you think of the quadratic term as the interaction of time with time. Then, as in all interactions, the coefficient for the main effect for Time will be its relation with outcome when the other variable in the interaction (i.e., Time) is 0.
- Thus, the regression coefficient for linear Time changes depending on where you center the time and timesq terms. Often, you will want to center these Time terms at the mean of Time. If you center Time at its average value, the regression coefficient for the linear time term will be the AVERAGE linear change over time. 
- You may also often want to center time at the last FU to compare groups at the last time point. In this case, the main effect for time will reflect the slope at that last time point.
 
## Homework 3 - 09/05

Run the quadratic growth curve model using “weeks” as the time variable. Center at week 52. Include condition as a moderator of week and weeksq. Include a random effect (RE) for the intercept, week, and weeksq, but drop the RE for weeksq if the model won't run. Drop the condition x weeksq interaction if it is not significant. Then compare the model fit (using AIC) to the model we ran in class using index1c5 as the time variable.

**SPSS**

```
* STEP 1. Restructure dataset. 

VARSTOCASES
  /MAKE YBOCS FROM YBOCpre YBOC4 YBOC16 YBOC26 YBOC52
  /MAKE BDI FROM BDIpre BDI4 BDI16 BDI26 BDI52
  /INDEX=Index1(5)
  /KEEP=Subject# Age condition HAMTOT Male0
  /NULL=KEEP.
```

```
* STEP 2. Recode Index1 into Week. 

RECODE Index1 (MISSING=SYSMIS) (1=0) (2=4) (3=16) (4=26) (5=52) INTO week.
EXECUTE.
```

```
* STEP 3. Center week at last assessment and create quadratic term. 
COMPUTE weekc52=week-52.
EXECUTE.

COMPUTE weekc52sq=weekc52 ** 2.
EXECUTE.
```

```
* STEP 4. Run model... but final Hessian matrix is not positive definite! 

MIXED YBOCS WITH weekc52 weekc52sq condition
  /CRITERIA=DFMETHOD(SATTERTHWAITE) CIN(95) MXITER(1000) MXSTEP(100) SCORING(1)
    SINGULAR(0.000000000001) HCONVERGE(0.00000001, RELATIVE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0,
    ABSOLUTE)
  /FIXED=weekc52 weekc52sq condition weekc52*condition weekc52sq*condition | SSTYPE(3)
  /METHOD=REML
  /PRINT=SOLUTION TESTCOV
  /RANDOM=INTERCEPT weekc52 weekc52sq | SUBJECT(Subject#) COVTYPE(UN)
  /REPEATED=weekc52 | SUBJECT(Subject#) COVTYPE(ID).
```

```{r, echo = FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/carolinelee78/mlmbookdown/main/_figures/IMG11.png")
```

```
* STEP 5. Drop highest order random effect as final Hessian matrix was not positive definite. Remember that you can't drop lower order terms; if you're going to drop, you drop from the highest order down; also, don't drop terms that are part of your hypothsis. 

MIXED YBOCS WITH weekc52 weekc52sq condition
  /CRITERIA=DFMETHOD(SATTERTHWAITE) CIN(95) MXITER(1000) MXSTEP(100) SCORING(1)
    SINGULAR(0.000000000001) HCONVERGE(0.00000001, RELATIVE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0,
    ABSOLUTE)
  /FIXED=weekc52 weekc52sq condition weekc52*condition weekc52sq*condition | SSTYPE(3)
  /METHOD=REML
  /PRINT=SOLUTION TESTCOV
  /RANDOM=INTERCEPT weekc52 | SUBJECT(Subject#) COVTYPE(UN) * Note that we dropped random effect for weekc52sq
  /REPEATED=weekc52 | SUBJECT(Subject#) COVTYPE(ID).
```

```{r, echo = FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/carolinelee78/mlmbookdown/main/_figures/IMG12.png")
```

```
* STEP 6. Drop condition*weekssq as it is not significant. 

MIXED YBOCS WITH weekc52 weekc52sq condition
  /CRITERIA=DFMETHOD(SATTERTHWAITE) CIN(95) MXITER(1000) MXSTEP(100) SCORING(1)
    SINGULAR(0.000000000001) HCONVERGE(0.00000001, RELATIVE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0,
    ABSOLUTE)
  /FIXED=weekc52 weekc52sq condition weekc52*condition | SSTYPE(3) * Note that we dropped fixed effect for condition*weekssq interaction 
  /METHOD=REML
  /PRINT=SOLUTION TESTCOV
  /RANDOM=INTERCEPT weekc52 | SUBJECT(Subject#) COVTYPE(UN)
  /REPEATED=weekc52 | SUBJECT(Subject#) COVTYPE(ID).
```

```{r, echo = FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/carolinelee78/mlmbookdown/main/_figures/IMG13.png")
```

**Interpretation of** _**weekc52**_ **Main Effect** 

The linear decrease in YBOCS in condition 0 at week 52 is not significant. 

**Interpretation of** _**weekc52sq**_ **Main Effect** 

There is a significant curvilinear change in YBOCS in condition 0 at week 52. 

**Interpretation of** _**condition**_ **Main Effect** 

Difference in YBOCS across conditions at week 52 is not significant. 

**Interpretation of** _**week52**_ $\times$ _**YBOCS**_ **Interaction Effect** 

The change in differences across conditions in terms of YBOCS over time is significant. 

--- 

**R**

```{r, warning = FALSE, results= 'hide'}
library(haven)
library(lme4)
library(dplyr)
```

```{r}
DF1 <- read_sav("Restructured_OCDMediationYBOCSBDI1inS.sav")
DF1$week <- dplyr::recode(DF1$Index1, `1` = 0,
                                      `2` = 4,
                                      `3` = 16,
                                      `4` = 26, 
                                      `5` = 52)

DF1$Subject <- DF1$`Subject#`

model1 <- lmer(YBOC ~ week + condition + week*condition + (1 + week || Subject), data = DF1)
summary(model1, corr = FALSE)
 
```

# weekc52 weekc52sq condition weekc52*condition weekc52sq*condition 


```{r}
DF1$week52 <- DF1$week - 52 

DF1$week52sq <- DF1$week52^2

model3 <- lmer(YBOC ~ week52 + week52sq + condition + week52*condition + week52sq*condition + (1 | week52), data = DF1)

summary(model3, corr = FALSE)
```

Now try to graph the model using index1c5. Graph 2 models: the one WITH the condition x index1c5sq included, and the one WITHOUT condition x index1c5sq. Remember the trick you have to use to graph when the time variable is a transformed variable (you have to use the TRANSFORMED variable in the excel spreadsheet, not the raw 1,2,3,4,5 variable).

## Nested Models

A smaller model (one with fewer parameters) is nested within a larger model if the larger model has ALL the parameters included in the smaller model, plus at least 1 additional parameter. Or, a smaller model is nested within a larger model if you can derive the smaller model by putting 1 or more restrictions on the larger model.

 - "Parameters" includes both fixed effects and random effects (variance components).
 - You can **statistically** compare **nested** models for relative “goodness of fit” using the difference in the -2LL. 
 - The difference in the -2LL of the 2 models is a chi-square whose degrees of freedom is the difference in the number of parameters estimated by the 2 models. 
 - Note that ML estimation must be used if your comparisons between models involve ANY differences in fixed effects (in your predictors/covariates) including differences in fixed effects and variance components 
 - However, if your models involve different fixed effects (except additional effects in the full model, since the full model by definition has fixed effects that are not in the nested model), you cannot compare them with -2LL. You must use AIC/BIC. 

REML can be used to compare models that differ *only* on variance components (e.g., random effects and covariances). However, these models can also be tested using ML, so you can just default to using ML for all comparisons. 

 - For all likelihood ratio tests (i.e., tests using the difference in the -2LL), regardless of using ML or REML, the 2-tailed p-value should be divided by 2 (i.e., use a 1 tailed test) to obtain the correct p-value (Berkhof & Snijders, 2001), basically because the -2LL of the restricted model is always larger than the -2LL of the fuller model (i.e., it is a unidirectional test).
 - This website can perform one tailed $\chi^2$ tests so that we don't need to divide by 2: [Daniel Soper One-Tailed $\chi^2$ Test Calculator](https://www.danielsoper.com/statcalc/calculator.aspx?id=11)




