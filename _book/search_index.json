[["applying-mlm-to-longitudinal-data.html", "Chapter 3 Applying MLM to Longitudinal Data 3.1 Overview 3.2 Homework 2 - 08/31 3.3 Quadratic Growth Curve Models 3.4 Homework 3 - 09/05 3.5 Homework 3.5 - 02/15 3.6 Nested Models", " Chapter 3 Applying MLM to Longitudinal Data 3.1 Overview Getting the “right” growth curve is critical since it can lead to misleading results if it is incorrect. So, graph the outcome data and make sure you understand the “growth curve” for individuals. Use “Plots” in “Univariate ANOVA” to look at what the growth curve looks like. Also, MLM assumes multivariate normality, so we want to be sure that our data approaches normality (robust standard errors are available in Generalized Linear mixed models, which we will talk about later in the semester). They are robust to violations of multivariate normality. Also check for outliers. Use scatterplots and frequencies to look for outliers, skewness, normality. Level-2 equations predict between person differences in the level-1 parameters (e.g., intercept and slope) in the level-1 growth curve model. Thus, the level-2 equations can be used to test if individual characteristics (e.g., treatment condition, demographic information, personality traits, etc.) predict these level-1 (within person) relations, answering questions like: is treatment condition significantly related to slope of change over time (b1). If you have a priori hypotheses (like a treatment condition effect that you want to test), you can go directly to testing that. If you do not have a priori hypotheses, you usually look at the variance of the random effect for the slope, and if it is significant, then that means that there is significant variability in the slope between different people, so it is worthwhile to investigate predictors of that variability. How do you test for variability in the slope? You can test the significance of \\(\\mu_{1i}\\). When you substitute the level 2 equations into the level 1 model, you get the composite model that is estimated using maximum likelihood (ML) estimation. Note that since there is an interaction between Time and Condition, the main effects for Time and Condition do not reflect the “average” effect of Time or Condition, but rather the effect of one when the other is 0. Thus, if Time is “centered” at assessment 1 (baseline), then Timec1=0 is the baseline assessment and the effect of condition is thus the Condition effect at baseline, which you hope to be non-significant. The effect of Time in this model is the slope for those in Condition=0 (hopefully you do not code condition =1 or 2, since then the slope of Time (the main effect) is nonsensical). One often runs the model twice, once for each treatment condition equaling 0, to get the slope (and significance) of Time for both treatment conditions. You can also run the model with treatment condition coded -.5, .5 to get the “average” effect of Time across both treatment conditions. This latter coding is not often done when treatment condition is the level 2 variable, since researchers rarely want to know about the average of placebo and treatment. It is more frequently done when the level 2 predictor of slope is a variable like gender, when you want to get the average slope across genders. One also often runs the model with Time centered at various assessments to obtain the significance of treatment condition at the various assessments. Thus, if the variable Index1 codes the 4 assessments as 1,2,3,4, one might run the model alternately with Time centered at baseline (Timec1=Index1-1), at post-treatment (Timec3=Index1-3), and at FU (Timec4=Index1-4). Proof that using Timec4 will give you condition differences at follow-up: \\[ Y = b_0 + b_1*Cond + b_2*Timec4 + b_3*Cond*Timec4 \\] \\[ Y (Timec4 = 0) = b_0 + b_1*Cond + b_2*0 + b_3*Cond*0\\] \\[= b_0 + b_1*Cond\\] So the main effect for condition when time is centered at assessment 4 is the effect of condition at assessment 4 (FU; follow-up). Note the difference between that and the prediction of Y (using this same coding of time) at time 3: \\[ Y(Timec4 = 1) = b_0 + b_1*Cond + b_2*(-1) + b_3*(-1)*Cond \\] \\[ = (b_0-b_2)+(b_1-b_3)*Cond \\] 3.2 Homework 2 - 08/31 Use the restructured OCD data file to rerun the analysis, using week (weeks from pre) as the time variable instead of index1. Use either a series of “computes” or use a “recode” to create the week variable. The “pre” assessment (index1=1) should be coded week=0. The week4 assessment (index1=2) should be coded week=4, etc. Then use condition, week, and condition x week as the fixed effects, and include a random effect for the intercept and for week (always use an unstructured covariance matrix for the random effects). Compare the results from this model to the results from the model we did in class. SPSS *STEP 1. Recode index1 into week as the time variable. RECODE Index1 (1=0) (2=4) (3=16) (4=26) (5=52) (MISSING=SYSMIS) INTO week. EXECUTE. * STEP 2. Center week at week52. COMPUTE week52=week-52. * STEP 3. Run model. MIXED YBOC WITH condition week52 /CRITERIA=DFMETHOD(SATTERTHWAITE) CIN(95) MXITER(1000) MXSTEP(100) SCORING(1) SINGULAR(0.000000000001) HCONVERGE(0.00000001, RELATIVE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0, ABSOLUTE) /FIXED=condition week52 condition*week52 | SSTYPE(3) /METHOD=REML /PRINT=SOLUTION TESTCOV /RANDOM=INTERCEPT week52 | SUBJECT(Subject#) COVTYPE(UN) /REPEATED=week | SUBJECT(Subject#) COVTYPE(ID). Interpretation of week52 Main Effect There is a significant difference on the effect of time on YBOC (decline for when condition = 0). Interpretation of condition Main Effect Centering at assessment 52, there’s no significant effect of condition. Interpretation of the Interaction Effect The change in differences across conditions in terms of YBOCS over time is significant. Interpretation of Covariance Parameters .563 is the variability of the individual scores around the estimated average of their conditions at the last time point (UN1.1). An individual’s average slope does vary around the average slope (UN2.2 – slope random effect – within each condition. Deviation for the average slope). The higher the slope, the higher the intercept for each individual on average as this is a positive value for (UN2.1 - covariance). R library(haven) library(lme4) ## Loading required package: Matrix library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union DF1 &lt;- read_sav(&quot;Restructured_OCDMediationYBOCSBDI1inS.sav&quot;) DF1$week &lt;- dplyr::recode(DF1$Index1, `1` = 0, `2` = 4, `3` = 16, `4` = 26, `5` = 52) DF1$Subject &lt;- DF1$`Subject#` DF1$week52 &lt;- DF1$week - 52 model2 &lt;- lmer(YBOC ~ week52 + condition + week52*condition + (1 + week52 | Subject), data = DF1) ## boundary (singular) fit: see help(&#39;isSingular&#39;) summary(model2, corr = FALSE) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: YBOC ~ week52 + condition + week52 * condition + (1 + week52 | ## Subject) ## Data: DF1 ## ## REML criterion at convergence: 483.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.5887 -0.3930 0.1551 0.5829 2.4698 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Subject (Intercept) 0.5907514 0.76860 ## week52 0.0001433 0.01197 1.00 ## Residual 0.2134546 0.46201 ## Number of obs: 285, groups: Subject, 62 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 1.993824 0.171175 11.648 ## week52 -0.021916 0.003283 -6.676 ## condition 0.477315 0.238725 1.999 ## week52:condition 0.009844 0.004559 2.159 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) 3.3 Quadratic Growth Curve Models A quadratic model of change includes a quadratic effect of time (i.e., as \\(time^2\\), otherwise known as the quadratic rate of change, or the quadratic slope for time) in addition to an intercept and a linear effect of time (i.e., as \\(time^1\\), otherwise known as the linear rate of change, or the linear slope for time). A quadratic effect of time implies a change in the linear rate of change over time, such that the linear time slope either accelerates (speeds up) or decelerates (slows down) over time. The figure below illustrates how positive or negative linear or quadratic effects of time can work together to create many possible patterns of nonlinear change. First, the top left shows how a positive linear effect of time and a positive quadratic effect of time create an accelerating positive function, or a mean trajectory in which the rate of increase speeds up over time. Second, the top right shows how a positive linear effect of time and a negative quadratic effect of time create a decelerating positive function, or a mean trajectory in which the rate of increase slows down over time instead. Third, the bottom left shows how a negative linear effect of time and a positive quadratic effect of time create a decelerating negative function, or a mean trajectory in which the rate of decrease slows down over time. Finally, the bottom right shows how a negative linear effect of time and a negative quadratic effect of time create an accelerating negative function, or a mean trajectory in which the rate of decrease speeds over time. We cannot determine just from its sign if a quadratic effect creates acceleration or deceleration. A positive quadratic effect of time can make a positive linear effect of time more positive (acceleration), or it can make a negative linear effect of time less negative (deceleration). Similarly, a negative quadratic effect of time can make a negative linear effect of time more negative (acceleration). Furthermore, just like any other predictor that varies over time, a quadratic effect of time can have both a fixed effect and a random effect. That is, a fixed quadratic effect of time in the model for the means implies a change in the linear rate of change on average. More specifically, Typically, in a treatment outcome study, the improvement in outcome over time levels off. Hence, we need to be able to model growth as curvilinear. We can do this by looking at the pattern of means across time, within condition, to get an idea of what the change in the means over time looks like. - Remember that this is the pattern of means for the non-missing data, so it may not reflect MLM growth curves. We can test for a quadratic curve by adding a “time squared” term to the level-1 MLM model. Note that we have a level-2 equation for every level 1 parameter, so with a quadratic growth curve, you will have 3 level-2 equations, adding the level-2 equation for the timesq term. The level 2 equation for the quadratic term does not have to have the same predictors that the linear time term has, but if it does have predictors, they must be included as predictors of linear time and the intercept. This is necessary to have all subcomponents of the interaction included in the model. Be sure that the squared term uses the exact linear term, squared. Don’t use timec1 for the linear term and timec5-squared for the squared term. Otherwise, the meaning of the some of the coefficients (e.g., main effects) can become problematic. Even higher level terms (e.g., cubic, quartic, etc.) can also be added if needed. 3.3.1 Meaning of the Linear vs. Quadratic Terms When using a quadratic growth curve model, the regression coefficient for the linear term is the tangent to the curve at Time=0. This makes sense when you think of the quadratic term as the interaction of time with time. Then, as in all interactions, the coefficient for the main effect for Time will be its relation with outcome when the other variable in the interaction (i.e., Time) is 0. Thus, the regression coefficient for linear Time changes depending on where you center the time and timesq terms. Often, you will want to center these Time terms at the mean of Time. If you center Time at its average value, the regression coefficient for the linear time term will be the AVERAGE linear change over time. You may also often want to center time at the last FU to compare groups at the last time point. In this case, the main effect for time will reflect the slope at that last time point. Centering is just as important in quadratic models as in linear models. Be sure to use the same variable for the linear and quadratic terms (centered at the same time point). Then, the “condition” effect is the effect of treatment condition when Time equals 0. 3.4 Homework 3 - 09/05 Run the quadratic growth curve model using “weeks” as the time variable. Center at week 52. Include condition as a moderator of week and weeksq. Include a random effect (RE) for the intercept, week, and weeksq, but drop the RE for weeksq if the model won’t run. Drop the condition x weeksq interaction if it is not significant. Then compare the model fit (using AIC) to the model we ran in class using index1c5 as the time variable. SPSS * STEP 1. Restructure dataset. VARSTOCASES /MAKE YBOCS FROM YBOCpre YBOC4 YBOC16 YBOC26 YBOC52 /MAKE BDI FROM BDIpre BDI4 BDI16 BDI26 BDI52 /INDEX=Index1(5) /KEEP=Subject# Age condition HAMTOT Male0 /NULL=KEEP. * STEP 2. Recode Index1 into Week. RECODE Index1 (MISSING=SYSMIS) (1=0) (2=4) (3=16) (4=26) (5=52) INTO week. EXECUTE. * STEP 3. Center week at last assessment and create quadratic term. COMPUTE weekc52=week-52. EXECUTE. COMPUTE weekc52sq=weekc52 ** 2. EXECUTE. * STEP 4. Run model... but final Hessian matrix is not positive definite! MIXED YBOCS WITH weekc52 weekc52sq condition /CRITERIA=DFMETHOD(SATTERTHWAITE) CIN(95) MXITER(1000) MXSTEP(100) SCORING(1) SINGULAR(0.000000000001) HCONVERGE(0.00000001, RELATIVE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0, ABSOLUTE) /FIXED=weekc52 weekc52sq condition weekc52*condition weekc52sq*condition | SSTYPE(3) /METHOD=REML /PRINT=SOLUTION TESTCOV /RANDOM=INTERCEPT weekc52 weekc52sq | SUBJECT(Subject#) COVTYPE(UN) /REPEATED=weekc52 | SUBJECT(Subject#) COVTYPE(ID). * STEP 5. Drop highest order random effect as final Hessian matrix was not positive definite. Remember that you can&#39;t drop lower order terms; if you&#39;re going to drop, you drop from the highest order down; also, don&#39;t drop terms that are part of your hypothsis. MIXED YBOCS WITH weekc52 weekc52sq condition /CRITERIA=DFMETHOD(SATTERTHWAITE) CIN(95) MXITER(1000) MXSTEP(100) SCORING(1) SINGULAR(0.000000000001) HCONVERGE(0.00000001, RELATIVE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0, ABSOLUTE) /FIXED=weekc52 weekc52sq condition weekc52*condition weekc52sq*condition | SSTYPE(3) /METHOD=REML /PRINT=SOLUTION TESTCOV /RANDOM=INTERCEPT weekc52 | SUBJECT(Subject#) COVTYPE(UN) * Note that we dropped random effect for weekc52sq /REPEATED=weekc52 | SUBJECT(Subject#) COVTYPE(ID). * STEP 6. Drop condition*weekssq as it is not significant. MIXED YBOCS WITH weekc52 weekc52sq condition /CRITERIA=DFMETHOD(SATTERTHWAITE) CIN(95) MXITER(1000) MXSTEP(100) SCORING(1) SINGULAR(0.000000000001) HCONVERGE(0.00000001, RELATIVE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0, ABSOLUTE) /FIXED=weekc52 weekc52sq condition weekc52*condition | SSTYPE(3) * Note that we dropped fixed effect for condition*weekssq interaction /METHOD=REML /PRINT=SOLUTION TESTCOV /RANDOM=INTERCEPT weekc52 | SUBJECT(Subject#) COVTYPE(UN) /REPEATED=weekc52 | SUBJECT(Subject#) COVTYPE(ID). Interpretation of weekc52 Main Effect The linear decrease in YBOCS in condition 0 at week 52 is not significant. Why would this be? Because the linear tangent to the quadratic change at week 52 would be a flat line, not a significant decrease. Interpretation of weekc52sq Main Effect There is a significant curvilinear change in YBOCS in condition 0 at week 52. Interpretation of condition Main Effect Difference in YBOCS across conditions at week 52 is not significant. Interpretation of week52 \\(\\times\\) YBOCS Interaction Effect The change in differences across conditions in terms of YBOCS over time is significant. R library(haven) library(lme4) library(dplyr) DF1 &lt;- read_sav(&quot;Restructured_OCDMediationYBOCSBDI1inS.sav&quot;) DF1$week &lt;- dplyr::recode(DF1$Index1, `1` = 0, `2` = 4, `3` = 16, `4` = 26, `5` = 52) DF1$Subject &lt;- DF1$`Subject#` DF1$week52 &lt;- DF1$week - 52 DF1$week52sq &lt;- DF1$week52^2 model3 &lt;- lmer(YBOC ~ week52 + (week52^2) + condition + week52*condition + (1 + week52 | Subject), data = DF1) ## boundary (singular) fit: see help(&#39;isSingular&#39;) summary(model3, corr = FALSE) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: YBOC ~ week52 + (week52^2) + condition + week52 * condition + ## (1 + week52 | Subject) ## Data: DF1 ## ## REML criterion at convergence: 483.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.5887 -0.3930 0.1551 0.5829 2.4698 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Subject (Intercept) 0.5907514 0.76860 ## week52 0.0001433 0.01197 1.00 ## Residual 0.2134546 0.46201 ## Number of obs: 285, groups: Subject, 62 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 1.993824 0.171175 11.648 ## week52 -0.021916 0.003283 -6.676 ## condition 0.477315 0.238725 1.999 ## week52:condition 0.009844 0.004559 2.159 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) Now try to graph the model using index1c5. Graph 2 models: the one WITH the condition x index1c5sq included, and the one WITHOUT condition x index1c5sq. Remember the trick you have to use to graph when the time variable is a transformed variable (you have to use the TRANSFORMED variable in the excel spreadsheet, not the raw 1,2,3,4,5 variable). Excel Model 1 (with condition*index1c5sq) Model 2 (without condition*index1c5sq) 3.5 Homework 3.5 - 02/15 Here is the description of the attached file: This is data from a 2 group study (Prolonged exposure (PE, coded PE0=0)) vs. control (coded PE0=1)) on PTSD symptoms (CPSSIE, which is the DV). Use Index1 as your time variable. Model a quadratic growth curve. Are there treatment condition differences in CPSSIE at the last assessment? Graph the model. What is the average linear decrease (slope) in symptoms per Unit of index1 for those in the PE condition? SPSS * STEP 1. Create centered variable and quadratic term. COMPUTE Index1c23=Index1 - 23. EXECUTE. COMPUTE Index1c23sq=Index1c23 ** 2. EXECUTE. * STEP 2. Run model with unstructured. MIXED CPSSIE WITH Index1c23 Index1c23sq pe0 /CRITERIA=DFMETHOD(SATTERTHWAITE) CIN(95) MXITER(100) MXSTEP(10) SCORING(1) SINGULAR(0.000000000001) HCONVERGE(0, ABSOLUTE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0.000001, ABSOLUTE) /FIXED=Index1c23 Index1c23sq pe0 Index1c23*pe0 Index1c23sq*pe0 | SSTYPE(3) /METHOD=ML /PRINT=SOLUTION TESTCOV /RANDOM=INTERCEPT Index1c23 Index1c23sq | SUBJECT(SubjectID) COVTYPE(UN) /REPEATED=Index1c23 | SUBJECT(SubjectID) COVTYPE(UN). * STEP 3. Did not converge, so going down the list of Model Selection Table. * Toeplitz - Heterogeneous - did not converge. * Toeplitz - did not converge. * AR1 Heterogeneous - did not converge. * AR1 - converged! MIXED CPSSIE WITH Index1c23 Index1c23sq pe0 /CRITERIA=DFMETHOD(SATTERTHWAITE) CIN(95) MXITER(100) MXSTEP(10) SCORING(1) SINGULAR(0.000000000001) HCONVERGE(0, ABSOLUTE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0.000001, ABSOLUTE) /FIXED=Index1c23 Index1c23sq pe0 Index1c23*pe0 Index1c23sq*pe0 | SSTYPE(3) /METHOD=ML /PRINT=SOLUTION TESTCOV /RANDOM=INTERCEPT Index1c23 Index1c23sq | SUBJECT(SubjectID) COVTYPE(UN) /REPEATED=Index1c23 | SUBJECT(SubjectID) COVTYPE(AR1). Are there treatment condition differences in CPSSIE at the last assessment? Graph the model. The main effect for pe0 indicates that the difference in CPSSIE across conditions at the last assessment is significant. What is the average linear decrease (slope) in symptoms per Unit of index1 for those in the PE condition? * STEP 1. Centering time (index1) at its average value, the regression coefficient for the linear time term will be the AVERAGE linear change over time. DESCRIPTIVES VARIABLES=Index1 /STATISTICS=MEAN. * Average is 13.35, so centering linear and quadratic terms at 13.35. COMPUTE Index1c13.35=Index1 - 13.35. EXECUTE. COMPUTE Index1c13.35sq= Index1c13.35 ** 2. EXECUTE. * STEP 2. Run model. * Did not converge with unstructured, going down the list. * Converged with AR1. MIXED CPSSIE WITH Index1c13.35 Index1c13.35sq pe0 /CRITERIA=DFMETHOD(SATTERTHWAITE) CIN(95) MXITER(100) MXSTEP(10) SCORING(1) SINGULAR(0.000000000001) HCONVERGE(0, ABSOLUTE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0.000001, ABSOLUTE) /FIXED=Index1c13.35 Index1c13.35sq pe0 Index1c13.35*pe0 Index1c13.35sq*pe0 | SSTYPE(3) /METHOD=ML /PRINT=SOLUTION TESTCOV /RANDOM=INTERCEPT Index1c13.35 Index1c13.35sq | SUBJECT(SubjectID) COVTYPE(UN) /REPEATED=Index1c13.35 | SUBJECT(SubjectID) COVTYPE(AR1). The average linear decrease in CPSSIE for those in the PE condition (pe0 = 0) is significant. This is because the linear tangent to the quadratic change on average shows a significant negative linear trend. 3.6 Nested Models A smaller model (one with fewer parameters) is nested within a larger model if the larger model has ALL the parameters included in the smaller model, plus at least 1 additional parameter. Or, a smaller model is nested within a larger model if you can derive the smaller model by putting 1 or more restrictions on the larger model. “Parameters” includes both fixed effects and random effects (variance components). You can statistically compare nested models for relative “goodness of fit” using the difference in the -2LL. The difference in the -2LL of the 2 models is a chi-square whose degrees of freedom is the difference in the number of parameters estimated by the 2 models. Note that ML estimation must be used if your comparisons between models involve ANY differences in fixed effects (in your predictors/covariates) including differences in fixed effects and variance components However, if your models involve different fixed effects (except additional effects in the full model, since the full model by definition has fixed effects that are not in the nested model), you cannot compare them with -2LL. You must use AIC/BIC. REML can be used to compare models that differ only on variance components (e.g., random effects and covariances). However, these models can also be tested using ML, so you can just default to using ML for all comparisons. For all likelihood ratio tests (i.e., tests using the difference in the -2LL), regardless of using ML or REML, the 2-tailed p-value should be divided by 2 (i.e., use a 1 tailed test) to obtain the correct p-value (Berkhof &amp; Snijders, 2001), basically because the -2LL of the restricted model is always larger than the -2LL of the fuller model (i.e., it is a unidirectional test). This website can perform one tailed \\(\\chi^2\\) tests so that we don’t need to divide by 2: Daniel Soper One-Tailed \\(\\chi^2\\) Test Calculator "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
